{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "light-keras-pix2pix.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1yuepxemk46j6_4z56w_pascIs83hVwYz",
          "timestamp": 1532185357236
        },
        {
          "file_id": "1TL0ZICfl3Xi4kBRTo5t8XL_JONFAkhwI",
          "timestamp": 1532171456940
        },
        {
          "file_id": "1fIEHXX_u7UpQ1dhZvmxcV5IScLiBIlXb",
          "timestamp": 1531588315092
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lKJyRUilph1Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5cefe487-8174-46fb-cc9c-c1ee8c8b8c42",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532217010868,
          "user_tz": -180,
          "elapsed": 58534,
          "user": {
            "displayName": "Zaid Alyafeai",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117000778514273508213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-07-21 23:49:14--  https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz\n",
            "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.189.73\n",
            "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.189.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2165283376 (2.0G) [application/x-gzip]\n",
            "Saving to: ‘edges2shoes.tar.gz’\n",
            "\n",
            "edges2shoes.tar.gz   71%[=============>      ]   1.45G  35.5MB/s    eta 17s    "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "edges2shoes.tar.gz  100%[===================>]   2.02G  35.6MB/s    in 57s     \n",
            "\n",
            "2018-07-21 23:50:11 (36.5 MB/s) - ‘edges2shoes.tar.gz’ saved [2165283376/2165283376]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Tva8R4PPy8o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!tar -xvzf edges2shoes.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLPjqkp8bnFu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "\n",
        "    def load_data(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path = glob('./%s/%s/*' % (self.dataset_name, data_type))\n",
        "        \n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs_A = []\n",
        "        imgs_B = []\n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "\n",
        "            h, w, _ = img.shape\n",
        "            _w = int(w/2)\n",
        "            img_A, img_B = img[:, _w:, :], img[:, :_w, :]\n",
        "\n",
        "            img_A = scipy.misc.imresize(img_A, self.img_res)\n",
        "            img_B = scipy.misc.imresize(img_B, self.img_res)\n",
        "\n",
        "            # If training => do random flip\n",
        "            if not is_testing and np.random.random() < 0.5:\n",
        "                img_A = np.fliplr(img_A)\n",
        "                img_B = np.fliplr(img_B)\n",
        "\n",
        "            imgs_A.append(img_A)\n",
        "            imgs_B.append(img_B)\n",
        "\n",
        "        imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "        imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "        return imgs_A, imgs_B\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path = glob('./%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(len(path) / batch_size)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch = path[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img in batch:\n",
        "                img = self.imread(img)\n",
        "                h, w, _ = img.shape\n",
        "                half_w = int(w/2)\n",
        "                img_A = img[:, half_w:, :]\n",
        "                img_B = img[:, :half_w, :]\n",
        "\n",
        "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
        "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img_A = np.fliplr(img_A)\n",
        "                        img_B = np.fliplr(img_B)\n",
        "\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "\n",
        "    def imread(self, path):\n",
        "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BsyQ6_rZCvO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa961a19-9c88-44a4-e9a0-72769c6a695d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532219230655,
          "user_tz": -180,
          "elapsed": 2848,
          "user": {
            "displayName": "Zaid Alyafeai",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117000778514273508213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import keras.backend as K"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pcbAkZ9lazD0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Pix2Pix():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 256\n",
        "        self.img_cols = 256\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        # Configure data loader\n",
        "        self.dataset_name = 'edges2shoes'\n",
        "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                      img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2**4)\n",
        "        self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 20\n",
        "        self.df = 20\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='mse',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        #-------------------------\n",
        "        # Construct Computational\n",
        "        #   Graph of Generator\n",
        "        #-------------------------\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Input images and their conditioning images\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # By conditioning on B generate a fake version of A\n",
        "        fake_A = self.generator(img_B)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Discriminators determines validity of translated images / condition pairs\n",
        "        valid = self.discriminator([fake_A, img_B])\n",
        "\n",
        "        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
        "        self.combined.compile(loss=['mse', 'mae'],\n",
        "                              loss_weights=[1, 100],\n",
        "                              optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
        "            \"\"\"Layers used during downsampling\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.8)(d)\n",
        "            return d\n",
        "\n",
        "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = BatchNormalization(momentum=0.8)(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n",
        "\n",
        "        # Image input\n",
        "        d0 = Input(shape=self.img_shape)\n",
        "\n",
        "        # Downsampling\n",
        "        d1 = conv2d(d0, self.gf, bn=False)\n",
        "        d2 = conv2d(d1, self.gf*2)\n",
        "        d3 = conv2d(d2, self.gf*4)\n",
        "        d4 = conv2d(d3, self.gf*8)\n",
        "        d5 = conv2d(d4, self.gf*8)\n",
        "        d6 = conv2d(d5, self.gf*8)\n",
        "        d7 = conv2d(d6, self.gf*8)\n",
        "\n",
        "        # Upsampling\n",
        "        u1 = deconv2d(d7, d6, self.gf*8)\n",
        "        u2 = deconv2d(u1, d5, self.gf*8)\n",
        "        u3 = deconv2d(u2, d4, self.gf*8)\n",
        "        u4 = deconv2d(u3, d3, self.gf*4)\n",
        "        u5 = deconv2d(u4, d2, self.gf*2)\n",
        "        u6 = deconv2d(u5, d1, self.gf)\n",
        "\n",
        "        u7 = UpSampling2D(size=2)(u6)\n",
        "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
        "\n",
        "        return Model(d0, output_img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.8)(d)\n",
        "            return d\n",
        "\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # Concatenate image and conditioning image by channels to produce input\n",
        "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
        "\n",
        "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
        "        d2 = d_layer(d1, self.df*2)\n",
        "        d3 = d_layer(d2, self.df*4)\n",
        "        d4 = d_layer(d3, self.df*8)\n",
        "\n",
        "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "        return Model([img_A, img_B], validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Condition on B and generate a translated version\n",
        "                fake_A = self.generator.predict(imgs_B)\n",
        "\n",
        "                # Train the discriminators (original images = real / generated = Fake)\n",
        "                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
        "                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "                # -----------------\n",
        "                #  Train Generator\n",
        "                # -----------------\n",
        "\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
        "\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "                # If at save interval => save generated image samples\n",
        "                if batch_i % sample_interval == 0:\n",
        "                    # Plot the progress\n",
        "                    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
        "                                                                        batch_i, self.data_loader.n_batches,\n",
        "                                                                        d_loss[0], 100*d_loss[1],\n",
        "                                                                        g_loss[0],\n",
        "                                                                        elapsed_time))\n",
        "                    self.sample_images(epoch, batch_i)\n",
        "            print('Saving the model ..., ')\n",
        "            self.generator.save('drive/models/keras_'+str(epoch)+'.h5')\n",
        "            \n",
        "\n",
        "    def sample_images(self, epoch, batch_i):\n",
        "        r, c = 3, 3\n",
        "\n",
        "        imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n",
        "        fake_A = self.generator.predict(imgs_B)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Condition', 'Generated', 'Original']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[i])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4s-s1pHn1uT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "gan = Pix2Pix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUQInktCQYn5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "gan.train(15, batch_size=4, sample_interval=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Izk4d571EQ0u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "gan.generator.save('keras.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBJqpglDY5Sc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2DwKsLSLrDT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "166e0744-8277-4bfe-e7b9-e1e5f02bf85d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532219349245,
          "user_tz": -180,
          "elapsed": 10577,
          "user": {
            "displayName": "Zaid Alyafeai",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117000778514273508213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir model\n",
        "!tensorflowjs_converter --input_format keras keras.h5 model/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q4toGU90r_D0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2694b3e0-50a2-4b2d-d18d-b0f64c5c9983",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532217266921,
          "user_tz": -180,
          "elapsed": 1658,
          "user": {
            "displayName": "Zaid Alyafeai",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117000778514273508213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "group10-shard1of1  group17-shard1of1  group23-shard1of1  group5-shard1of1\r\n",
            "group11-shard1of1  group18-shard1of1  group24-shard1of1  group6-shard1of1\r\n",
            "group12-shard1of1  group19-shard1of1  group25-shard1of1  group7-shard1of1\r\n",
            "group13-shard1of1  group1-shard1of1   group26-shard1of1  group8-shard1of1\r\n",
            "group14-shard1of1  group20-shard1of1  group2-shard1of1\t group9-shard1of1\r\n",
            "group15-shard1of1  group21-shard1of1  group3-shard1of1\t model.json\r\n",
            "group16-shard1of1  group22-shard1of1  group4-shard1of1\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NId5RP1ExA6t",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uXcqrvcxC-U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsehfXTAxFPl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f3c52e54-0019-4680-8abd-252e725ebe56",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532217417311,
          "user_tz": -180,
          "elapsed": 15803,
          "user": {
            "displayName": "Zaid Alyafeai",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "117000778514273508213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "# Work around misordering of STREAM and STDIN in Jupyter.\n",
        "# https://github.com/jupyter/notebook/issues/3159\n",
        "prompt = !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass(prompt[0] + '\\n\\nEnter verification code: ')\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "\n",
            "Enter verification code: ··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8qvoEjO8xIEm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uKQnMrfMkkUq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir drive/models"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}